# -*- coding: utf-8 -*-
"""UseCase1_Fraud_Detection_Wilhelmus_Medhavi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mwgHWXNZvbIs3cjD44hlJ5o-4vUJlTej

Klasifikasi supervised Use Case Fraud Detection (https://www.kaggle.com/competitions/ieee-fraud-detection/overview)

Tahapan pengerjaan:

1. Data Understanding
    a. Data Description: laod data, cek ukuran data, cek informasi data, statistika deskriptif, jumlah missing value, drop kolom banyak missing value
    b. EDA : melakukan EDA univariat maupun multivariat terhadap kolom numerik dan kategori
2. Data Preparation : data cleaning missing value berdasarkan jenis kategorik atau numerik
3. Feature Engineering
    a. Feature Selection : mengurangi fitur untuk modelling dengan gabungan voting beberapa metode yaitu Korelasi, KBest, MutualInfo, RFE, MRMR, Crammers V, Encoder
    b. Feature Transformation : transformasi fitur, split train test, undersampling dengan tomeklinks dan randomundersampler
4. Modelling
    a. Model Selection : menentukan model yang akan dipilih berdasarkan parameter default
    b. Model Tuning : mengatur hyperparameter untuk mendapatkan model yang lebih sesuai
5. Model Evaluation - Backtesting
    melakukan pengecekan dengan menerapkan prediksi model pada data backtesting

# **Data Understanding**
"""

# Importing modul

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as mnso
import random
import mrmr
import optuna
import lightgbm as lgb
import category_encoders as ce

from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss
from sklearn.preprocessing import PowerTransformer
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from collections import Counter
from scipy.stats import chi2_contingency
from mrmr import mrmr_classif
from sklearn.feature_selection import mutual_info_classif,SelectKBest

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import (
    ExtraTreesClassifier,
    RandomForestClassifier,
    AdaBoostClassifier
)

from scipy.stats import ttest_ind
from sklearn.impute import KNNImputer, SimpleImputer
from scipy.stats import skew,skewtest,spearmanr,pearsonr,normaltest
from sklearn.preprocessing import RobustScaler,OneHotEncoder, OrdinalEncoder
from sklearn.feature_selection import mutual_info_classif, RFE, SelectKBest, chi2, f_classif
from sklearn.pipeline import Pipeline
from sklearn.pipeline import make_pipeline
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import roc_curve, precision_recall_curve,make_scorer,accuracy_score, average_precision_score, precision_recall_curve
from sklearn.model_selection import train_test_split, cross_validate, KFold, cross_val_score, RandomizedSearchCV, GridSearchCV, cross_val_predict, StratifiedKFold


import warnings
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)

### Define the function

def cramers_v_eval(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2, p, dof, expected = chi2_contingency(confusion_matrix)
    n = confusion_matrix.sum().sum()
    phi2 = chi2 / n
    r, k = confusion_matrix.shape
    phi2corr = max(0, phi2 - ((k - 1)*(r - 1)) / (n - 1))  # bias correction
    rcorr = r - ((r - 1)**2 / (n - 1))
    kcorr = k - ((k - 1)**2 / (n - 1))
    cramers_v = np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))
    return chi2, p, cramers_v

def describe_all_columns(df):
    summary = []

    for col in df.columns:
        total = len(df)
        null_count = df[col].isnull().sum()
        null_percent = (null_count / total) * 100
        nunique = df[col].nunique(dropna=True)
        mode_val = df[col].mode().iloc[0] if not df[col].mode().empty else None
        dtype = df[col].dtype

        desc = {
            'Column': col,
            'Type': str(dtype),
            'Null Count': null_count,
            'Null %': f"{null_percent:.2f}%",
            'Unique Values': nunique,
            'Most Frequent': mode_val
        }

        if pd.api.types.is_numeric_dtype(df[col]):
            stats = df[col].describe()
            desc.update({
                'Mean': stats['mean'],
                'Std': stats['std'],
                'Min': stats['min'],
                '25%': stats['25%'],
                '50% (Median)': stats['50%'],
                '75%': stats['75%'],
                'Max': stats['max']
            })

        summary.append(desc)

    return pd.DataFrame(summary)

def mutual_info_scores(df, target_col):
    # Split features and label
    # X = df.drop(columns=[target_col])
    X = df.select_dtypes(include=[np.number]).drop(columns=[target_col], errors='ignore')
    y = df[target_col]

    # Compute mutual information scores
    mi_scores = mutual_info_classif(X, y, random_state=42)

    # Build a dataframe of results
    mi_df = pd.DataFrame({
        'Feature': X.columns,
        'Mutual Information': mi_scores
    })

    # Sort descending
    mi_df = mi_df.sort_values(by='Mutual Information', ascending=False).reset_index(drop=True)
    return mi_df

def calc_VIF(x):
  vif= pd.DataFrame()
  vif['variables']=x.columns
  vif["VIF"]=[variance_inflation_factor(x.values,i) for i in range(x.shape[1])]
  return(vif)

def calculate_vif(df):
    # Add a constant for intercept
    df_with_const = add_constant(df)

    vif_data = pd.DataFrame()
    vif_data["Feature"] = df.columns
    vif_data["VIF"] = [
        variance_inflation_factor(df_with_const.values, i + 1)  # +1 to skip intercept
        for i in range(df.shape[1])
    ]
    return vif_data

def remove_high_vif_features(df, threshold=10):
    """
    Iteratively remove features with VIF higher than threshold
    """
    while True:
        vif = calculate_vif(df)
        max_vif = vif['VIF'].max()
        if max_vif > threshold:
            drop_feature = vif.sort_values('VIF', ascending=False)['Feature'].iloc[0]
            print(f"Dropping '{drop_feature}' with VIF = {max_vif:.2f}")
            df = df.drop(columns=[drop_feature])
        else:
            break
    return df, calculate_vif(df)

# Fraud Detection

# tujuan: untuk memprediksi terjadinya kejadian fraud dengan menggunakan model supervised classification machine learning
# data train dan test menggunakan identity dan transaction, kemudian dicek kembali

# Load Dataset

# Data Train
train1 = pd.read_csv('train_identity.csv',sep=',')
train2 = pd.read_csv('train_transaction.csv',sep=',')


# Data Test
test1 = pd.read_csv('test_identity.csv',sep=',')
test2 = pd.read_csv('test_transaction.csv',sep=',')

# Data checking identity

train1

# Data checking transaction

train2

train1['TransactionID'].nunique()

train2['TransactionID'].nunique()

# Data testing checking

test1

# Data testing transaction checking

test2

"""## Data Description"""

# Melihat informasi dataset train

train1.info()

# Melihat informasi dataset train trx

train2.info()

# Melihat informasi dataset test trx

train1.info()

# Melihat informasi dataset test trx

train2.info()

# Melihat jumlah missing value train identity

mnso.bar(train1)

# Melihat jumlah missing value train trx

mnso.bar(train2)

# Melihat jumlah missing value train identity

mnso.bar(test1)

# Melihat jumlah missing value train trx

mnso.bar(test2)

# Check Missing Value

"""Metode penanganan missing value :

1. missing value ≤10% gunakan simple imputer yaitu median untuk kolom numerik dan modus untuk kolom kategorik <p>
2. missing value >10% dan ≤30%  gunakan KNNImputer atau <p>
3. missing value >30% drop kolom
"""

# Melihat jumlah persentase missing value - train identity

train1.isna().sum()/len(train1)

# Melihat jumlah persentase missing value - train trx

train2.isna().sum()/len(train2)

# Melihat jumlah persentase missing value - test identity

test1.isna().sum()/len(test1)

# Melihat jumlah persentase missing value - test trx

test2.isna().sum()/len(test2)

# Drop Feature > 30% missing

threshold = 0.3 # max 30% null

# filter data dengan treshold tersebut

train1_null_percentage = train1.isnull().mean()
train2_null_percentage = train2.isnull().mean()
test1_null_percentage = test1.isnull().mean()
test2_null_percentage = test2.isnull().mean()

train1_filtered = train1.loc[:, train1_null_percentage < threshold]
train2_filtered = train2.loc[:, train2_null_percentage < threshold]
test1_filtered = test1.loc[:, test1_null_percentage < threshold]
test2_filtered = test2.loc[:, test2_null_percentage < threshold]

train2['isFraud'].value_counts()

# Penggabungan data
# pembagian data train test dan backtesting

df_train = train2_filtered.join(train1_filtered, on='TransactionID', how='left',lsuffix='_1')
df_backtest = test2_filtered.join(test1_filtered, on='TransactionID', how='left',lsuffix='_1')

train_null_percentage = df_train.isnull().mean()
test_null_percentage = df_backtest.isnull().mean()

df_train = df_train.loc[:, train_null_percentage < threshold]
df_backtest = df_backtest.loc[:, test_null_percentage < threshold]

df_train

df_backtest

# Memisahkan data kategorik dan data numerik

df_num = df_train.select_dtypes('number')
df_cat = df_train.select_dtypes('object')

# Menampilkan statistika deskriptif secara keseluruhan pada data numerik

df_num.describe().T

# Menampilkan statistika deskriptif secara keseluruhan pada data kategorik

df_cat.describe().T

"""## Exploratory Data Analysis"""

# Melihat kolom numerik

df_num.columns

# Melihat kolom kategorik

df_cat.columns

"""EDA univariat setiap kolom numerik dengan bentuk : <p>
    a. histogram dan boxplot untuk tiap kolom<p>
    b. metrik statistik dasar untuk tiap kolom: mean, std, min, q1, q2, q3, iqr, max<p>
    c. nilai upper whisker (nilai maximum tanpa outlier) dan lower whisker (nilai minimum tanpa outlier) dari boxplot tiap kolom<p>
    d. medeteksi outlier (<q1-1.5*iqr | >q3+1.5*iqr); menghitung count dan list dari outlier tiap kolom<p>
    e. menghitung metrik skew dan lakukan skewtest untuk tiap kolom<p>
"""

# Melakukan EDA Univariat pada setiap kolom numerik dataset

# Membuat list nama kolom numerik
list_kontinu = list(df_num.columns)

# Membuat loop
iter = 1
for i in list_kontinu:
  print()
  print('Iterasi ke-',iter)
  print('Kolom : ',i)
  sns.histplot(df_num[i],kde=True)
  plt.axvline(np.mean(df_num[i]),c='green',linestyle='--')
  plt.axvline(np.percentile(df_num[i],25),c='red',linestyle='--')
  plt.axvline(np.percentile(df_num[i],50),c='red',linestyle='--')
  plt.axvline(np.percentile(df_num[i],75),c='red',linestyle='--')
  plt.show()
  sns.boxplot(data=df_num, x=df_num[i])
  plt.show()

  print()
  print('Skew Value : ',skew(df_num[i]))
  print('Skew Test Value : ',skewtest(df_num[i]))
  print(df_num[i].describe())

  print()
  Q1 = np.percentile(df_num[i],25)
  Q2 = np.percentile(df_num[i],50)
  Q3 = np.percentile(df_num[i],75)
  IQR = Q3-Q1

  upper_f = Q3 + 1.5 * IQR
  lower_f = Q1 - 1.5 * IQR

  upper_w = np.max(df_num[df_num[i] <= upper_f][i])
  lower_w = np.min(df_num[df_num[i] >= lower_f][i])
  outlier = list(df_num[(df_num[i] < lower_w) | (df_num[i] > upper_f)][i])

  print('nilai max di luar outlier = {}'.format(upper_w))
  print('nilai min di luar outlier = {}'.format(lower_w))
  print('jumlah outlier:{}'.format(len(outlier)))
  print('outlier:')
  print(outlier)
  iter+=1
  print()

"""EDA univariat untuk setiap kolom kategorikal dengan countplot untuk tiap kolom dan jumlah kategori unik, daftar kategori unik dan value counts tiap kategori unik<p>"""

# Melakukan EDA Univariat pada setiap kolom kategorikal dataset

# Membuat list nama kolom kategorikal
list_kategori = list(df_cat.columns)

# Membuat loop
iter = 1
for i in list_kategori:
  print()
  print('Iterasi ke-',iter)
  print('Kolom : ',i)
  sns.countplot(data = df_cat, x = i)
  plt.show()

  print('Jumlah kategori unik :', len(df_cat[i].unique()))
  print('Jumlah nilai kategori unik :', list(df_cat[i].unique()))
  print('Jumlah total nilai kategori unik :')
  print(df_cat[i].value_counts())
  iter+=1
  print()

"""EDA multivariat untuk tiap pasangan kolom numerik-numerik dengan lmplot antar kolom numerik dengan kolom 'label' sebagai hue"""

# Membuat list pasangan nama kolom numerikal
col1 = list(df_num.columns)
# col2 = list(df_num['isFraud'])

col1.remove('isFraud')

list_pasangan = []
for i in col1:
    list_temp = sorted([i,'isFraud'])
    if list_temp not in list_pasangan :
        list_pasangan.append(list_temp)

len(list_pasangan)

# Melakukan EDA Multivariat pada setiap pasangan kolom kategorikal dengan kolom label dataset

# Membuat list nama kolom kategorikal
list_kategorikal = list(df_cat.columns)

iter = 1
# Membuat loop
for i in list_kategorikal:
  print('iterasi ke-',iter)
  print('Hubungan label dengan kolom',i)
  sns.countplot(data=df_cat, x=i, hue=df_train['isFraud'])
  plt.show()
  pd.crosstab(df_cat[i], df_train['isFraud'], normalize='index').plot(kind='bar', stacked=True)
  plt.show()
  iter+=1
  print()

# Melakukan EDA Multivariat pada setiap pasangan kolom kontinu dengan kolom label dataset

# Membuat list nama kolom numerik
list_kontinu = list(df_num.columns)
list_kontinu.remove('isFraud')

# Membuat loop
iter=1
for i in list_kontinu:
  print()
  print('iterasi ke-',iter)
  print('Hubungan label dengan kolom',i)

  sns.boxplot(data=df_train, x='isFraud', y=i)
  plt.show()
  print()
  sns.pointplot(data=df_train, x='isFraud', y=i)
  plt.show()

  alp = 0.05
  group1 = df_train[(df_train['isFraud']==1)][i]
  group0 = df_train[(df_train['isFraud']==0)][i]
  ttest_coef,ttest_pval = ttest_ind(group1,group0,nan_policy='omit', alternative='two-sided')
  print('Nilai t-test :',ttest_coef)
  print('Nilai pvalue t-test :',ttest_pval)
  if ttest_pval < 0.05 :
    print('Dikarenakan nilai pvalue ttest < 0.05 maka terima H0')
    print('Kesimpulan : mean variabel numerik kelompok label 1 = mean variabel numerik kelompok label 0')
  else :
    print('Dikarenakan nilai pvalue ttest > 0.05 maka tolak H0')
    print('Kesimpulan : mean variabel numerik kelompok label 1 ≠ mean variabel numerik kelompok label 0')

  iter+=1
  print()

"""# **Data Preparation**

Metode penanganan missing value :

1. missing value ≤30% gunakan simple imputer yaitu median untuk kolom numerik dan modus untuk kolom kategorik <p>
2. missing value >30% drop kolom

Karena keseluruhan nilai missing value yang melebihi 30% sudah di drop, maka tindakan yang dilakukan hanyalah menggunakan simple imputer. Tahapannya adalah dengan memisahkan data kontinu dan kategorikal dahulu, kemudian dilakukan onehotencoder dan diimpute.
"""

# Mengecek kembali informasi dan missing value data

df_train.info()

df_train.isna().sum()/len(df_train)

df_cat.columns

# Check kolom null pada categorical
null_percent_cat = df_cat.isnull().mean() * 100
null_percent_cat = null_percent_cat.sort_values(ascending=False)

# Tampilkan hasil
null_percent_cat = null_percent_cat[null_percent_cat > 0]
null_percent_cat

df_num.columns

# Check kolom null pada numerical
null_percent_num = df_num.isnull().mean() * 100
null_percent_num = null_percent_num.sort_values(ascending=False)

# Tampilkan hasil
null_percent_num = null_percent_num[null_percent_num > 0]
null_percent_num

# Mengubah feature kategorikal menjadi numerik dengan ordinal encoder
oe = OrdinalEncoder(handle_unknown ='use_encoded_value',unknown_value=np.nan)
df_cat_oe = pd.DataFrame(oe.fit_transform(df_cat),columns = oe.feature_names_in_)
df_cat_oe

# Menyatukan kedua jenis tipe data menjadi satu dataset

df_concat_imp = pd.concat([df_cat_oe, df_num], axis = 1)
df_concat_imp

# Memasukan nilai null kedalam dataset dengan kondisi yang telah didefinisikan
# impute mode : 4 kolom kategorikal transformed
# impute median : 178 kolom numeric

# Mengambil semua kolom yang terdapat null
cat_with_null = null_percent_cat.index.tolist()
num_with_null = null_percent_num.index.tolist()

# Menggunakan imputer mode
df_modus = df_concat_imp[cat_with_null]
Mod_imp = SimpleImputer(strategy='most_frequent')
Mod_imp_fit = pd.DataFrame(Mod_imp.fit_transform(df_modus), columns = Mod_imp.feature_names_in_)
df_concat_imp[cat_with_null] = Mod_imp_fit

# Menggunakan imputer median
df_mean = df_concat_imp[num_with_null]
Mean_imp = SimpleImputer(strategy='median')
Mean_imp_fit = pd.DataFrame(Mean_imp.fit_transform(df_mean), columns = Mean_imp.feature_names_in_)
df_concat_imp[num_with_null] = Mean_imp_fit

# Melakukan inverse numerik terhadap data kategorikal

df_concat_imp[oe.feature_names_in_] = oe.inverse_transform(df_concat_imp[oe.feature_names_in_])
df_concat_imp

# Check null kembali
null_percent = df_concat_imp.isnull().mean() * 100
null_percent = null_percent.sort_values(ascending=False)

null_percent = null_percent[null_percent > 0]
null_percent

# # Checkpoint

# df_concat_imp.to_csv("df_concat_imp.csv",sep='+')
# df_train.to_csv("df_train.csv",sep='+')
# df_backtest.to_csv("df_backtest.csv",sep='+')

"""# **Feature Engineering**

## Feature Selection
"""

# Dikarenakan fitur terlalu banyak maka akan dikurangi dengan menggunakan beberapa metode pemilihan fitur yakni
# Korelasi, Kbest, Mutual

# pisahkan data numeric dan categori

# Numerical
df_num = df_concat_imp.select_dtypes('number')
df_num = df_num.drop(['TransactionID_1','TransactionDT'],axis=1)
# Categorical
df_cat = df_concat_imp.select_dtypes('object')

df_num

# FEATURE NUMERCIAL

### Heatmap Correlation

# Create the heatmap
correlation_matrix = df_num.corr()
cor_flag_trx = correlation_matrix['isFraud'].sort_values()
cor_flag_trx = pd.DataFrame(cor_flag_trx.drop('isFraud'))

plt.figure(figsize = (12,10))
sns.heatmap(correlation_matrix, cmap = 'coolwarm',annot=True, fmt=".2f", square=True, linewidths=.5)
plt.show()

cor1 = pd.DataFrame(cor_flag_trx.head(15)).reset_index()
cor2 = pd.DataFrame(cor_flag_trx.tail(15)).reset_index()

corelation = pd.concat([cor1,cor2])
corelation = pd.DataFrame(corelation.rename({'index':'Feature','isFraud':'isFraud_Corr'},axis=1)).reset_index(drop=True)
corelation

### Mutual Information

mif = mutual_info_scores(df_num, target_col='isFraud')
mif_list = list(mif.head(30)['Feature'])
mif_sort = mif.sort_values(by = 'Mutual Information', ascending=False)
mif_sort

mif_get = mif_sort.head(30)
mif_get

### KBEST

# Select KBEST
selector = SelectKBest(f_classif, k=30)
X = df_concat_imp.select_dtypes(include=[np.number]).drop(columns=['TransactionID_1','TransactionDT','isFraud'], errors='ignore')
y = df_concat_imp['isFraud']
X_new = selector.fit_transform(X, y)

selected_columns = X.columns[selector.get_support()]
print("Selected Columns:")
print(selected_columns)
kbest = list(selected_columns)


# Create DataFrame to inspect scores and p-values
kbest_results = pd.DataFrame({
    'Feature': X.columns,
    'Score': selector.scores_,
    'P-value': selector.pvalues_,
    'Selected': selector.get_support()
})

# Sort by score (descending)
kbest_results = kbest_results.sort_values(by='Score', ascending=False).reset_index(drop=True)

# Show all results
kbest_results

kbest_get = kbest_results.head(30)
kbest_get

### RFE

rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=30)
X = df_concat_imp.select_dtypes(include=[np.number]).drop(columns=['TransactionID_1','TransactionDT','isFraud'], errors='ignore')
y = df_concat_imp['isFraud']
rfe.fit(X,y)

print(rfe.support_)
print(rfe.ranking_)

# Initialize all importance values with NaN
importance_full = np.full(shape=X.shape[1], fill_value=np.nan)
# Fill importance values only for selected features
importance_full[rfe.support_] = rfe.estimator_.feature_importances_

X.columns[rfe.support_]
rfe_results = pd.DataFrame({
    'Feature': X.columns,
    'Selected': rfe.support_,
    'Ranking': rfe.ranking_,
    'Importance': importance_full
}).sort_values(by='Importance', ascending=False)

rfe_results['Ranking'].sort_values().unique()
rfe_res = pd.DataFrame(rfe_results[rfe_results['Ranking'] <= 2]['Feature'])
rfe_res_list = list(rfe_res['Feature'])
rfe_res_list

rfe_results = rfe_results.sort_values(by='Ranking').reset_index(drop=True)
rfe_results

rfe_results_sort = rfe_results.sort_values(by = 'Importance', ascending=False)
rfe_results_sort = rfe_results_sort.reset_index(drop=True)
rfe_get = rfe_results_sort.head(30)
rfe_get

plt.figure(figsize=(12,6))
sns.barplot(x='Importance', y='Feature', data=rfe_get.sort_values('Importance'), palette="viridis")
plt.title("RFE Ranking of Features")
plt.xlabel("RFE Ranking (1 = most important)")
plt.ylabel("Features")
plt.show()

# checkpoint

# rfe_get.to_csv("rfe_get.csv",sep='+')
# kbest_get.to_csv("kbest_get.csv",sep='+')
# mif_get.to_csv("mif_get.csv",sep='+')
# corelation.to_csv("corelation.csv",sep='+')
# mrmr_get.to_csv("mrmr_get.csv",sep='+')

# df_concat_imp = pd.read_csv("df_concat_imp.csv",sep='+')
# df_train = pd.read_csv("df_train.csv",sep='+')
# df_backtest = pd.read_csv("df_backtest.csv",sep='+')

# rfe_get = pd.read_csv("rfe_get.csv",sep='+')
# kbest_get = pd.read_csv("kbest_get.csv",sep='+')
# mif_get = pd.read_csv("mif_get.csv",sep='+')
# corelation = pd.read_csv("corelation.csv",sep='+')

### MRMR Classification

X_ = df_concat_imp.select_dtypes(include=[np.number]).drop(columns=['TransactionID_1','TransactionDT','isFraud'], errors='ignore')
y_ = df_concat_imp['isFraud']
selected_features = mrmr_classif(
    X=X_,
    y=y_,
    K=30
)

# karena mrmr hanya menampilkan nama feature saja maka untuk scoring dapat menggunakan subtitusi mutual info
mi_scores = mutual_info_classif(X_[selected_features], y_, discrete_features='auto')
importance_df = pd.DataFrame({
    'Feature': selected_features,
    'mrmr_mutual_info_score': mi_scores
}).sort_values(by='mrmr_mutual_info_score', ascending=False)

importance_df

mrmr_get = importance_df.reset_index(drop=True)
mrmr_get

# Voting fitur yang akan digunakan

corr = corelation['Feature'].tolist()
mif = mif_get['Feature'].tolist()
rfe = rfe_get['Feature'].tolist()
kbest = kbest_get['Feature'].tolist()
mrmr = mrmr_get['Feature'].tolist()

combined = corr + mif + rfe + kbest + mrmr
Counter(combined)

selected_keys_num = [k for k, v in Counter(combined).items() if v >= 3]
selected_keys_num

len(selected_keys_num)

# FEATURE CATEGORICAL

df_cat

### CRAMMERS V

results = []

for col in df_cat:
    chi2, p, cramers_v_score = cramers_v_eval(df_cat[col], df_concat_imp['isFraud'])
    results.append({
        'Feature': col,
        'Chi2': round(chi2, 4),
        'p-value': round(p, 4),
        "Cramér's V": round(cramers_v_score, 4),
        'Significant (p < 0.05)': p < 0.05
    })

# Create DataFrame
cramers_df = pd.DataFrame(results).sort_values(by="Cramér's V", ascending=False).reset_index(drop=True)
top_features_cram = cramers_df[cramers_df["Cramér's V"] > 0.1]['Feature'].tolist()

print(top_features_cram)
cramers_df

### Target Encoder

features = df_cat.columns.tolist()
encoder = ce.TargetEncoder(cols=features)
encoder.fit(df_cat, df_concat_imp['isFraud'])
encoded_df = encoder.transform(df_cat)
encoded_df['isFraud'] = df_concat_imp['isFraud']

# Menghitung korelasi pearson feature dengan label
correlation_df = encoded_df.corr()['isFraud'].reset_index()
correlation_df.columns = ['Feature', 'Correlation']

# Filter feature dengan nilai korelasi diatas 0,1
correlation_df_filtered = correlation_df[(correlation_df['Feature'] != 'isFraud') & (correlation_df['Correlation'].abs() > 0.1)]
correlation_df_filtered = correlation_df_filtered.sort_values(by='Correlation', ascending=False).reset_index(drop=True)
selected_features_enc = correlation_df_filtered['Feature'].tolist()

print(selected_features_enc)
correlation_df_filtered

# Voting fitur yang akan digunakan

combined = top_features_cram + selected_features_enc
Counter(combined)

# Final Fitur
cat_feature_get = [k for k, v in Counter(combined).items()]

final_feature = selected_keys_num + cat_feature_get
final_feature

# Mengecek multikolinearitas dengan VIF pada final dataset

# Drop multikolinear numerik > 10
X = df_concat_imp[final_feature].select_dtypes(include=[np.number]).copy()
X = X.drop(columns=['isFraud'], errors='ignore')
X_cleaned, final_vif = remove_high_vif_features(X)

list_final_vif = final_vif['Feature'].tolist()
print()

final_vif

"""## Feature Transformation"""

# Checkpoint
# df_concat_final.to_csv("df_concat_final.csv",sep='+')

df_concat_final = pd.read_csv("df_concat_final.csv",sep='+')
df_concat_final

# Dataset final dilihat kembali atributnya sebelum dilakukan pemisahan train test split dan transformasi

# df_concat_final = df_concat_imp[list_final_vif + cat_feature_get + ['isFraud']]
df_concat_final = df_concat_final[['V18', 'V43', 'V38', 'V39', 'V79', 'V52', 'V44', 'V87', 'V86', 'V45',
    'card3', 'ProductCD', 'card6', 'isFraud']]

df_concat_final.info()

# Melakukan pemisahan variabel X dan Y

X = df_concat_final.drop(['isFraud'],axis=1)
Y = df_concat_final['isFraud']

# Melihat jumlah data label

Y.value_counts(normalize=False)

# Melihat rasio label 0 dan 1

Y.value_counts(normalize=True)

# Melakukan pemisahan train test split

X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,stratify=Y,random_state=42)

# Melihat ukuran dataset dari train test

print('X train :',X_train.shape)
print('y train :',y_train.shape)
print('X test :',X_test.shape)
print('y test :',y_test.shape)

X_train

# Data Train

# Memishakan data kategorikal dan kontinu

X_train_num = X_train.select_dtypes('number')
X_train_cat = X_train.select_dtypes('object')

# Check sebaran distribusi data

list_kontinu = X_train_num.columns.tolist()
iter = 1
for i in list_kontinu:
  print()
  print('Iterasi ke-',iter)
  print('Kolom : ',i)
  sns.histplot(X_train_num[i],kde=True)
  plt.axvline(np.mean(X_train_num[i]),c='green',linestyle='--')
  plt.axvline(np.percentile(X_train_num[i],25),c='red',linestyle='--')
  plt.axvline(np.percentile(X_train_num[i],50),c='red',linestyle='--')
  plt.axvline(np.percentile(X_train_num[i],75),c='red',linestyle='--')
  plt.show()

  Q1 = np.percentile(X_train_num[i],25)
  Q2 = np.percentile(X_train_num[i],50)
  Q3 = np.percentile(X_train_num[i],75)
  IQR = Q3-Q1

  upper_f = Q3 + 1.5 * IQR
  lower_f = Q1 - 1.5 * IQR

  upper_w = np.max(X_train_num[X_train_num[i] <= upper_f][i])
  lower_w = np.min(X_train_num[X_train_num[i] >= lower_f][i])
  outlier = list(X_train_num[(X_train_num[i] < lower_w) | (X_train_num[i] > upper_f)][i])

  # Hitung delta
  mean_val = np.mean(X_train_num[i])
  median_val = np.percentile(X_train_num[i],50)
  delta = mean_val - median_val

    # Interpretasi
  if delta > 0:
        skew_direction = "Positive Skew (Right Skew)"
  elif delta < 0:
        skew_direction = "Negative Skew (Left Skew)"
  else:
        skew_direction = "No Skew (Symmetric)"
  print()
  print(len(outlier))
  print(len(X_train_num[i]))
  print('jumlah outlier:{}'.format(len(outlier)/len(X_train_num[i])*100))
  print('Nilai median : ',median_val)
  print('Nilai mean : ',mean_val)
  print('Delta (Mean - Median): {}'.format(delta))
  print('Skew Value : ',skew(X_train_num[i]))
  print('Skew Test Value : ',skewtest(X_train_num[i]))
  print('Skewness: {}'.format(skew_direction))
  print(X_train_num[i].describe())
  iter+=1
  print()

"""1. Min-Max Scaler = Jika data tidak banyak outlier, ingin skala [0,1]
2. Standard Scaler	= Jika data normal (distribusi Gaussian)
3. Robust Scaler = Jika banyak outlier
4. MaxAbs Scaler = Jika data sparse
5. Quantile Transformer = Jika distribusi ingin menjadi rapi/normal, sensitif terhadap outlier
6. Power Transformer = Jika distribusi ingin menyerupai rapi/normal, lebih tahan terhadap outlier

Berdasarkan bentuk distribusi dari feature numerical, ditentukan menggunakan jenis power transformer
"""

# Melakukan scaling

pt = PowerTransformer(method='yeo-johnson')
X_train_num_sc = pd.DataFrame(pt.fit_transform(X_train_num),columns = pt.get_feature_names_out(X_train_num.columns))
X_train_num_sc

# Melakukan encoder

ohe = OneHotEncoder(handle_unknown='ignore')
X_train_cat_enc = pd.DataFrame(ohe.fit_transform(X_train_cat).toarray(),columns =ohe.get_feature_names_out(X_train_cat.columns))
X_train_cat_enc

# Menggabungkan kedua jenis data menjadi satu

X_train_concat = pd.concat([X_train_num_sc, X_train_cat_enc], axis = 1)
X_train_concat

# Dikarenakan data terlalu imbalance (96:4) maka perlu dilakukan tindakan undersampling secara hati-hati dengan kombinasi
# Tindakan undersampling hanya dilakukan pada data train, yakni tomeklinks dan nearmiss

# 1. Bersihkan data yang dekat boundary overlap dengan TomekLinks
tl = TomekLinks()
X_clean, y_clean = tl.fit_resample(X_train_concat, y_train)

# 2. Balance data dengan proporsi yang diinginkan menggunakan randomundersampling
rus = RandomUnderSampler(sampling_strategy=0.3, random_state=42)
X_resampled, y_resampled = rus.fit_resample(X_clean, y_clean)

# 2. Balance data dengan NearMiss, v1 memilih majority dengan rata-rata jarak terdekat terkecil ke minority
# nm = NearMiss(version=1)
# X_resampled, y_resampled = nm.fit_resample(X_clean, y_clean)

X_resampled

y_resampled.value_counts(normalize=False)



# Data Test

# Memisahkan data kategorikal dan kontinu data test

X_test_num = X_test.select_dtypes('number')
X_test_cat = X_test.select_dtypes('object')

# Melakukan scaling

X_test_num_sc = pd.DataFrame(pt.transform(X_test_num),columns = pt.get_feature_names_out(X_test_num.columns))
X_test_num_sc

# Melakukan encoder

X_test_cat_enc = pd.DataFrame(ohe.transform(X_test_cat).toarray(),columns =ohe.get_feature_names_out(X_test_cat.columns))
X_test_cat_enc

# Menggabungkan kedua jenis data menjadi satu

X_test_concat = pd.concat([X_test_num_sc, X_test_cat_enc], axis = 1)
X_test_concat

# Membuat dataframe feature final yang digunakan modeling

X_train_final = X_resampled.copy()
X_test_final = X_test_concat.copy()
y_train = y_resampled.copy()

# Melihat ukuran data train dan test final

print('X train final :',X_train_final.shape)
print('y train final :',y_train.shape)
print('X test final :',X_test_final.shape)
print('y test final :',y_test.shape)

### Checkpoint

# X_train_concat.to_csv("X_train_concat.csv",sep='+')
# X_test_concat.to_csv("X_test_concat.csv",sep='+')

# y_train.to_csv("y_train.csv",sep='+')
# y_test.to_csv("y_test.csv",sep='+')

# X_train_concat = pd.read_csv("X_train_concat.csv",sep='+')
# X_test_concat = pd.read_csv("X_test_concat.csv",sep='+')

# y_train = pd.read_csv("y_train.csv",sep='+')
# y_test = pd.read_csv("y_test.csv",sep='+')

X_train_concat = X_train_concat.drop('Unnamed: 0',axis=1)
X_test_concat = X_test_concat.drop('Unnamed: 0',axis=1)
y_train = y_train.drop('Unnamed: 0',axis=1)
y_test = y_test.drop('Unnamed: 0',axis=1)

X_train_final

"""# **Modelling**"""

# Mendeklarasikan fungsi yang dibutuhkan

def classif(estimator, x, y):
    y_pred = estimator.predict(x)
    print(classification_report(y, y_pred, labels=[1,0]))

def list_metrics_model(model, y_test, y_pred):
    from sklearn import metrics
    report = metrics.classification_report(y_test, y_pred, output_dict=True)

    # Extract precision, recall, etc
    preci = report['1']['precision']  # Precision class 1
    reca = report['1']['recall']      # Recall class 1
    tnr = report['0']['recall']        # Recall class 0 = True Negative Rate
    fpr = 1 - tnr                      # FPR = 1 - TNR
    npv = report['0']['precision']     # NPV = Precision class 0
    f1 = report['1']['f1-score']       # F1-score class 1
    acc = report['accuracy']           # Overall Accuracy

    output_list = [model, preci, reca, tnr, fpr, npv, f1, acc]
    return output_list

# Mendeklarasikan model-model klasifikasi (prioritas tahan outlier dan imbalance)

rf = RandomForestClassifier(random_state=42)
xg = XGBClassifier(random_state=42)
lg = LGBMClassifier(random_state=42)
iso = IsolationForest(contamination=0.05, random_state=42)

# Melakukan fitting

rf.fit(X_train_final, y_train)
y_train_pred_rf = rf.predict(X_train_final)
y_test_pred_rf = rf.predict(X_test_final)

xg.fit(X_train_final, y_train)
y_train_pred_xg = xg.predict(X_train_final)
y_test_pred_xg = xg.predict(X_test_final)

lg.fit(X_train_final, y_train)
y_train_pred_lg = lg.predict(X_train_final)
y_test_pred_lg = lg.predict(X_test_final)

# Membuat list metrics dan membuat dataframe tiap model

train_metric_rf = list_metrics_model('RF_train',y_train,y_train_pred_rf)
test_metric_rf = list_metrics_model('RF_test',y_test,y_test_pred_rf)
df_rf_train = pd.DataFrame(train_metric_rf).T
df_rf_test = pd.DataFrame(test_metric_rf).T
df_rf_train.columns=['model','precision','recall','TNR','FPR','NPV','f1_score','accuracy']
df_rf_test.columns=['model','precision','recall','TNR','FPR','NPV','f1_score','accuracy']

train_metric_xg = list_metrics_model('xg_train',y_train,y_train_pred_xg)
test_metric_xg = list_metrics_model('xg_test',y_test,y_test_pred_xg)
df_xg_train = pd.DataFrame(train_metric_xg).T
df_xg_test = pd.DataFrame(test_metric_xg).T
df_xg_train.columns=['model','precision','recall','TNR','FPR','NPV','f1_score','accuracy']
df_xg_test.columns=['model','precision','recall','TNR','FPR','NPV','f1_score','accuracy']

train_metric_lg = list_metrics_model('lg_train',y_train,y_train_pred_lg)
test_metric_lg = list_metrics_model('lg_test',y_test,y_test_pred_lg)
df_lg_train = pd.DataFrame(train_metric_lg).T
df_lg_test = pd.DataFrame(test_metric_lg).T
df_lg_train.columns=['model','precision','recall','TNR','FPR','NPV','f1_score','accuracy']
df_lg_test.columns=['model','precision','recall','TNR','FPR','NPV','f1_score','accuracy']

"""## Model Selection"""

# Memilih satu model terbaik untuk dilakukan tuning
# Menyatukan keseluruhan model dataframe dan melihat hasil prediksi

df_classification_all = pd.concat([df_rf_train,df_rf_test,
                                   df_xg_train,df_xg_test,
                                   df_lg_train,df_lg_test]).reset_index()
df_classification_all = df_classification_all.drop(['index'],axis=1)
df_classification_all

# Berdasarkan hasil perbandingan kesluruhan model klasifikasi default, akan dipilih satu model untuk dilakukan tunning optimasi
# Kriteria model : Recall, Precision, dan f1 score

# Model default yang dipilih adalah model yang mendekati kriteria tersebut
# Berdasarkan hasil tersebut secara keseluruhan belum ada memenuhi, dengan peringkat:
# 1. Random Forest
# 2. XGBoost
# 3. LGBM


# Meskipun berdasarkan hasil rf yang paling baik, namun untuk kondisi nyata sebaiknya menggunakan :
# 1. lgbm karena training jauh lebih cepat dan hemat memori
# 2. Xgboost karena cocok untuk imbalance dengan ahsil stabil dan kontrol overfit regularisasi

# Confusion matrix train

y_true = y_train
y_pred = y_train_pred_rf


confu_map = confusion_matrix(y_true, y_pred)
display = ConfusionMatrixDisplay(confusion_matrix=confu_map, display_labels=rf.classes_)
display.plot(cmap='Blues', values_format='d')

plt.title('Confusion Matrix Data Train RF')
plt.show()

# Confusion matrix test

y_true = y_test
y_pred = y_test_pred_rf


confu_map = confusion_matrix(y_true, y_pred)
display = ConfusionMatrixDisplay(confusion_matrix=confu_map, display_labels=rf.classes_)
display.plot(cmap='Blues', values_format='d')

plt.title('Confusion Matrix Data Test RF')
plt.show()

# Terlihat disini hasil recall sudah jauh membaik namun malah precision yang jatuh
# Hal ini menandakan model terlalu agresif untuk memberikan flag fraud pada transaksi normal

# Perlu dilakukan tuning kepada model random forest dengan tuning hyperparameter

"""## Model Tunning

1. Random Forest
"""

# Define Optuna objective function
def objective_rf(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'max_depth': trial.suggest_int('max_depth', 3, 30),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),
        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
        'class_weight': trial.suggest_categorical('class_weight', [{0:1, 1:2}, {0:1, 1:3}]),
        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),
        'random_state': 42,
        'n_jobs': -1
    }

    model = RandomForestClassifier(**params)

    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

    # Cross-validated predicted probabilities
    y_pred_proba = cross_val_predict(model, X_train_final, y_train,
                                     cv=cv, method='predict_proba', n_jobs=-1)[:, 1]

    # Adjust threshold
    threshold = trial.suggest_float('threshold', 0.35, 0.45)  # explore wider threshold range
    y_pred = (y_pred_proba >= threshold).astype(int)

    # Calculate metrics
    recall = recall_score(y_train, y_pred, pos_label=1)
    precision = precision_score(y_train, y_pred, pos_label=1)
    f1 = f1_score(y_train, y_pred, pos_label=1)

    # Print recall and precision per trial
    print(f"Trial {trial.number}: Recall={recall:.4f} | Precision={precision:.4f} | F1={f1:.4f} | Threshold={threshold:.3f}")

    # Custom condition
    if precision < 0.35 or recall < 0.7 :
        return 0
    return recall

# Run Optuna study
study_rf = optuna.create_study(direction="maximize", study_name="rf_opt")
study_rf.optimize(objective_rf, n_trials=100, show_progress_bar=True)

# Best RF parameters
print()
print("Best Random Forest Parameters:\n", study_rf.best_params)
print("Best Recall Score RF:", study_rf.best_value)

# Best Random Forest Parameters:
#  {'n_estimators': 688, 'max_depth': 25, 'min_samples_split': 7, 'min_samples_leaf': 19, 'max_features': 'log2', 'class_weight': {0: 1, 1: 3}, 'bootstrap': True, 'threshold': 0.35056013406495456}
# Best Recall Score RF: 0.7564988938053098

# Hasil tuning RF dengan best params

rf_best = RandomForestClassifier(
n_estimators = 688,
max_depth = 25,
min_samples_split = 7,
min_samples_leaf = 19,
max_features = 'log2',
class_weight = {0: 1, 1: 3},
bootstrap = True
)
rf_best.fit(X_train_final, y_train)
threshold_best = 0.35056013406495456


# Prediksi probabilitas
y_train_pred_proba = rf_best.predict_proba(X_train_final)[:, 1]
y_test_pred_proba = rf_best.predict_proba(X_test_final)[:, 1]

# Convert probabilitas jadi class
y_train_pred_rf_opt = (y_train_pred_proba >= threshold_best).astype(int)
y_test_pred_rf_opt = (y_test_pred_proba >= threshold_best).astype(int)

# Melihat hasil fitting model ada default pada data train

print(classification_report(y_train, y_train_pred_rf_opt, labels=[1,0]))

# Melihat hasil fitting model ada default pada data test

print(classification_report(y_test, y_test_pred_rf_opt, labels=[1,0]))

rf_best

# Confusion matrix train

y_true = y_train
y_pred = y_train_pred_rf_opt


confu_map = confusion_matrix(y_true, y_pred)
display = ConfusionMatrixDisplay(confusion_matrix=confu_map, display_labels=rf_best.classes_)
display.plot(cmap='Blues', values_format='d')

plt.title('Confusion Matrix Data Train')
plt.show()

# Confusion matrix test

y_true = y_test
y_pred = y_test_pred_rf_opt


confu_map = confusion_matrix(y_true, y_pred)
display = ConfusionMatrixDisplay(confusion_matrix=confu_map, display_labels=rf_best.classes_)
display.plot(cmap='Blues', values_format='d')

plt.title('Confusion Matrix Data Test')
plt.show()

"""2. LGBM"""

# Define Optuna untuk LightGBM
def objective_lgbm(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'max_depth': trial.suggest_int('max_depth', 3, 30),
        'num_leaves': trial.suggest_int('num_leaves', 20, 300),
        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'class_weight': trial.suggest_categorical('class_weight', [{0:1, 1:2}, {0:1, 1:3}]),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0),
        'random_state': 42,
        'n_jobs': -1
    }

    model = LGBMClassifier(**params)

    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

    # Cross-validated predicted probabilities
    y_pred_proba = cross_val_predict(
        model, X_train_final, y_train,
        cv=cv, method='predict_proba', n_jobs=-1
    )[:, 1]

    # Adjust threshold
    threshold = trial.suggest_float('threshold', 0.35, 0.45)
    y_pred = (y_pred_proba >= threshold).astype(int)

    # Calculate metrics
    recall = recall_score(y_train, y_pred, pos_label=1)
    precision = precision_score(y_train, y_pred, pos_label=1)
    f1 = f1_score(y_train, y_pred, pos_label=1)

    # Print recall, precision and F1 per trial
    print(f"Trial {trial.number}: Recall={recall:.4f} | Precision={precision:.4f} | F1={f1:.4f} | Threshold={threshold:.3f}")

    # Custom condition: penalize if precision or recall too low
    if precision < 0.35 or recall < 0.7:
        return 0
    return recall

# Run Optuna study
study_lgbm = optuna.create_study(direction="maximize", study_name="lgbm_opt")
study_lgbm.optimize(objective_lgbm, n_trials=100, show_progress_bar=True)

# Best LGBM parameters
print()
print("Best LightGBM Parameters:\n", study_lgbm.best_params)
print("Best Recall Score LGBM:", study_lgbm.best_value)

# Best LightGBM Parameters:
#  {'n_estimators': 235, 'max_depth': 11, 'num_leaves': 92, 'learning_rate': 0.0018569441371058333, 'min_child_samples': 83, 'subsample': 0.9380880555870716, 'colsample_bytree': 0.5095655532290463, 'class_weight': {0: 1, 1: 3}, 'reg_alpha': 4.358692586562104, 'reg_lambda': 1.208195283454296, 'threshold': 0.41428144574517584}
# Best Recall Score LGBM: 0.770879424778761

# Hasil tuning lgbm dengan best params

lgbm_best = LGBMClassifier(
    n_estimators=235,
    max_depth=11,
    num_leaves=92,
    learning_rate=0.0018569441371058333,
    min_child_samples=83,
    subsample=0.9380880555870716,
    colsample_bytree=0.5095655532290463,
    class_weight={0: 1, 1: 3},
    reg_alpha=4.358692586562104,
    reg_lambda=1.208195283454296,
    random_state=42,
    n_jobs=-1
)

# Fit model ke data training
lgbm_best.fit(X_train_final, y_train)

# Threshold hasil tuning
threshold_best = 0.41428144574517584

# Prediksi probabilitas
y_train_pred_proba = lgbm_best.predict_proba(X_train_final)[:, 1]
y_test_pred_proba = lgbm_best.predict_proba(X_test_final)[:, 1]

# Konversi probabilitas menjadi class label
y_train_pred_lgbm_opt = (y_train_pred_proba >= threshold_best).astype(int)
y_test_pred_lgbm_opt = (y_test_pred_proba >= threshold_best).astype(int)

# Melihat hasil fitting model ada default pada data train

print(classification_report(y_train, y_train_pred_lgbm_opt, labels=[1,0]))

# Melihat hasil fitting model ada default pada data test

print(classification_report(y_test, y_test_pred_lgbm_opt, labels=[1,0]))

lgbm_best

# Confusion matrix train

y_true = y_train
y_pred = y_train_pred_lgbm_opt


confu_map = confusion_matrix(y_true, y_pred)
display = ConfusionMatrixDisplay(confusion_matrix=confu_map, display_labels=rf_best.classes_)
display.plot(cmap='Blues', values_format='d')

plt.title('Confusion Matrix Data Train')
plt.show()

# Confusion matrix test

y_true = y_test
y_pred = y_test_pred_lgbm_opt


confu_map = confusion_matrix(y_true, y_pred)
display = ConfusionMatrixDisplay(confusion_matrix=confu_map, display_labels=rf_best.classes_)
display.plot(cmap='Blues', values_format='d')

plt.title('Confusion Matrix Data Test')
plt.show()

"""3. XGBoost"""

# Define Optuna untuk XGBoost

def objective_xgb(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'max_depth': trial.suggest_int('max_depth', 3, 30),
        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'gamma': trial.suggest_float('gamma', 0.0, 5.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0),
        'scale_pos_weight': trial.suggest_categorical('scale_pos_weight', [1, 2, 3]),
        'random_state': 42,
        'n_jobs': -1,
        'use_label_encoder': False,
        'eval_metric': 'logloss'
    }

    model = XGBClassifier(**params)

    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

    y_valid_pred = np.zeros(len(y_train))  # untuk simpan hasil validasi prediksi

    for train_idx, valid_idx in cv.split(X_train_final, y_train):
        X_tr, X_val = X_train_final.iloc[train_idx], X_train_final.iloc[valid_idx]
        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]

        model.fit(X_tr, y_tr)
        y_valid_pred[valid_idx] = model.predict_proba(X_val)[:, 1]

    # Adjust threshold
    threshold = trial.suggest_float('threshold', 0.35, 0.45)
    y_pred = (y_valid_pred >= threshold).astype(int)

    # Calculate metrics
    recall = recall_score(y_train, y_pred, pos_label=1)
    precision = precision_score(y_train, y_pred, pos_label=1)
    f1 = f1_score(y_train, y_pred, pos_label=1)

    # Print recall, precision and F1 per trial
    print(f"Trial {trial.number}: Recall={recall:.4f} | Precision={precision:.4f} | F1={f1:.4f} | Threshold={threshold:.3f}")

    # Custom condition: penalize if precision or recall too low
    if precision < 0.35 or recall < 0.7:
        return 0
    return recall

# Run Optuna study
study_xgb = optuna.create_study(direction="maximize", study_name="xgb_opt")
study_xgb.optimize(objective_xgb, n_trials=100, show_progress_bar=True)

# Best XGBoost parameters
print()
print("Best XGBoost Parameters:\n", study_xgb.best_params)
print("Best Recall Score XGBoost:", study_xgb.best_value)

# Best XGBoost Parameters:
#  {'n_estimators': 491, 'max_depth': 8, 'learning_rate': 0.0015890303741570824, 'subsample': 0.5590602734963631, 'colsample_bytree': 0.6410085896943774, 'gamma': 2.6426523641924575, 'reg_alpha': 0.5852074950974842, 'reg_lambda': 3.5062079775069144, 'scale_pos_weight': 3, 'threshold': 0.3727396211048535}
# Best Recall Score XGBoost: 0.7694275442477876

xgb_best = XGBClassifier(
    n_estimators=491,
    max_depth=8,
    learning_rate=0.0015890303741570824,
    subsample=0.5590602734963631,
    colsample_bytree=0.6410085896943774,
    gamma=2.6426523641924575,
    reg_alpha=0.5852074950974842,
    reg_lambda=3.5062079775069144,
    scale_pos_weight=3,
    random_state=42,
    n_jobs=-1,
    use_label_encoder=False,
    eval_metric='logloss'
)

# Fit model ke data training
xgb_best.fit(X_train_final, y_train)

# Threshold hasil tuning
threshold_best = 0.3727396211048535

# Prediksi probabilitas
y_train_pred_proba = xgb_best.predict_proba(X_train_final)[:, 1]
y_test_pred_proba = xgb_best.predict_proba(X_test_final)[:, 1]

# Konversi probabilitas menjadi class label
y_train_pred_xgb_opt = (y_train_pred_proba >= threshold_best).astype(int)
y_test_pred_xgb_opt = (y_test_pred_proba >= threshold_best).astype(int)

# Melihat hasil fitting model ada default pada data train

print(classification_report(y_train, y_train_pred_xgb_opt, labels=[1,0]))

# Melihat hasil fitting model ada default pada data test

print(classification_report(y_test, y_test_pred_xgb_opt, labels=[1,0]))

print(xgb_best)

# Confusion matrix train

y_true = y_train
y_pred = y_train_pred_xgb_opt


confu_map = confusion_matrix(y_true, y_pred)
display = ConfusionMatrixDisplay(confusion_matrix=confu_map, display_labels=rf_best.classes_)
display.plot(cmap='Blues', values_format='d')

plt.title('Confusion Matrix Data Train')
plt.show()

# Confusion matrix test

y_true = y_test
y_pred = y_test_pred_xgb_opt


confu_map = confusion_matrix(y_true, y_pred)
display = ConfusionMatrixDisplay(confusion_matrix=confu_map, display_labels=rf_best.classes_)
display.plot(cmap='Blues', values_format='d')

plt.title('Confusion Matrix Data Test')
plt.show()

"""# **Model Evaluation**

## Backtesting
"""

df_concat_final.columns.tolist()

# Mempersiapkan backtesting

# df_backtest = pd.read_csv("df_backtest.csv",sep='+')
df_test = df_backtest.drop('Unnamed: 0',axis=1).copy()
df_test = df_test[[
'V18',
 'V43',
 'V38',
 'V39',
 'V79',
 'V52',
 'V44',
 'V87',
 'V86',
 'V45',
 'card3',
 'ProductCD',
 'card6'
]]
df_test

# Melihat dataset
df_test.info()

df_test.isna().sum()

# Mengisi missing value/imputasi

# impute mode : categorical
# impute median : numerical

# Menggunakan imputer mode
df_modus = df_test[df_test.select_dtypes('object').columns]
Mod_imp = SimpleImputer(strategy='most_frequent')
Mod_imp_fit = pd.DataFrame(Mod_imp.fit_transform(df_modus), columns = Mod_imp.feature_names_in_)
df_test[df_test.select_dtypes('object').columns] = Mod_imp_fit

# Menggunakan imputer median
df_mean = df_test[df_test.select_dtypes('number').columns]
Mean_imp = SimpleImputer(strategy='median')
Mean_imp_fit = pd.DataFrame(Mean_imp.fit_transform(df_mean), columns = Mean_imp.feature_names_in_)
df_test[df_test.select_dtypes('number').columns] = Mean_imp_fit

# Memisahkan data kontinu dan kategorikal

df_test_num = df_test.select_dtypes('number')
df_test_cat = df_test.select_dtypes('object')

# Melakukan encoder

df_test_cat_enc = pd.DataFrame(ohe.transform(df_test_cat).toarray(),columns =ohe.get_feature_names_out(df_test_cat.columns))
df_test_cat_enc

# Melakukan scaling

df_test_num_sc = pd.DataFrame(pt.transform(df_test_num),columns =df_test_num.columns)
df_test_num_sc

# Menggabungkan kedua jenis data backtest menjadi satu dan menyiapkan data untuk backtest

backtest_final = pd.concat([df_test_num_sc, df_test_cat_enc], axis = 1)
backtest_final

## Hasil model yang akan digunakan

rf_best = RandomForestClassifier(
n_estimators = 688,
max_depth = 25,
min_samples_split = 7,
min_samples_leaf = 19,
max_features = 'log2',
class_weight = {0: 1, 1: 3},
bootstrap = True
)
rf_best.fit(X_train_final, y_train)
threshold_best = 0.35056013406495456

rf_best

threshold_best

# Prediksi hasil backtest
predicted_data = rf_best.predict(backtest_final)
predicted_proba = rf_best.predict_proba(backtest_final)[:,1]

# Apply threshold_best
predicted_data = (predicted_proba >= threshold_best).astype(int)

# konversi array menjadi dataframe
df_predicted = pd.DataFrame(data=predicted_data,columns=['predicted_label_isFraud'])
df_proba = pd.DataFrame(data=predicted_proba,columns=['probability_isFraud'])

df_predict_all = pd.concat([df_test_num_sc, df_test_cat_enc, df_proba, df_predicted], axis = 1)
df_predict_all

df_predicted.value_counts()

df_predicted.value_counts(normalize=True)

# Cek apakah threshold sudah benar

df_predict_all[(df_predict_all['predicted_label_isFraud']==1)].sort_values(by='probability_isFraud',ascending=True)

df_predict_all

df_predict_all.to_csv("df_predict_all.csv",sep='+')

# Model terbaik hasil tuning memiliki nilai recall tinggi (77%) dengan presisi rendah (6%) dalam mendeteksi fraud
# Sebagai konsekuensinya akan meningkat jumlah false positive berdasarkan consufion matrix
# Perlu dilakukan pendalaman lebih terkait fitur-fitur yang sudah ada maupun belum ada dalam model dalam memperbaiki model

